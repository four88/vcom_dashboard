{"version":3,"sources":["images/about.png","images/flowchart_datapipeline.png","images/azure_key.png","images/bi.svg","images/python.png","images/azure.png","images/react.png","components/Nav/Nav.js","components/Header/Header.js","components/About/About.js","components/Tools/Tools.js","components/Card/Card.js","components/Member/Member.js","components/Avatar/Avatar.js","components/Footer/Footer.js","components/Main/Main.js","components/Selection/Selection.js","components/Country/Country.js","components/Dashboard/Dashboard.js","components/CodeBlock/CodeBlock.js","components/Documentation/Documentation.js","components/App/App.js","reportWebVitals.js","index.js","images/gbenga.png","images/tj.png","images/four.png"],"names":["module","exports","__webpack_require__","p","Nav","react_default","a","createElement","className","react_router_dom","to","Header","_ref","heading","About","history","useHistory","alt","src","aboutImg","type","onClick","push","Tools","children","Card","cardImg","cardTitle","cardDesc","Member","Avatar","name","major","img","Footer","Main","title","Fragment","Selection","Country","country","toPath","Dashboard","CodeBlock","code","lang","default_highlight","language","style","tomorrowNightEighties","showLineNumbers","Documentation","codes","flowChart","href","azure","clasName","App","cardInfo","pyImg","desc","biImg","azureImg","reactImg","avatarInfo","fourImg","tjImg","gbengaImg","react_router","exact","path","map","item","width","height","frameborder","allowFullScreen","reportWebVitals","onPerfEntry","Function","e","then","bind","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","createRoot","document","getElementById","render","StrictMode","basename","process"],"mappings":"4EAAAA,EAAAC,QAAiBC,EAAAC,EAAuB,sDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,uECAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,0DCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,mDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,uDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,sDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,+HCGzB,SAASC,IAEtB,OACEC,EAAAC,EAAAC,cAAA,OAAKC,UAAU,OACbH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,aACZH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,aACZH,EAAAC,EAAAC,cAACE,EAAA,EAAI,CAACD,UAAU,YAAYE,GAAG,KAAG,kBAIpCL,EAAAC,EAAAC,cAAA,MAAIC,UAAU,aACZH,EAAAC,EAAAC,cAACE,EAAA,EAAI,CAACD,UAAU,YAAYE,GAAG,cAAY,cAI7CL,EAAAC,EAAAC,cAAA,MAAIC,UAAU,aACZH,EAAAC,EAAAC,cAACE,EAAA,EAAI,CAACD,UAAU,YAAYE,GAAG,SAAO,oBChBjC,SAASC,EAAMC,GAE3B,IADDC,EAAOD,EAAPC,QAGA,OAEER,EAAAC,EAAAC,cAAA,UAAQC,UAAU,UAChBH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,mBACXK,GAEHR,EAAAC,EAAAC,cAACH,EAAG,mCCTK,SAASU,IAEtB,IAAMC,EAAUC,cAMhB,OACEX,EAAAC,EAAAC,cAAA,WAASC,UAAU,SACjBH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,oBACbH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,aACbS,IAAI,QACJC,IAAKC,MAEPd,EAAAC,EAAAC,cAAA,OAAKC,UAAU,kBACbH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,kBAAgB,sBAG9BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,eAAa,gXAK1BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,eAAa,ySAM1BH,EAAAC,EAAAC,cAAA,UAAQC,UAAU,gBAChBY,KAAK,SACLC,QA5BgB,WACxBN,EAAQO,KAAK,gBA2BsB,sBClCxB,SAASC,EAAKX,GAE1B,IADDY,EAAQZ,EAARY,SAEA,OACEnB,EAAAC,EAAAC,cAAA,WAASC,UAAU,SACjBH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,gBAAc,gBAG5BH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,oBACZgB,ICTM,SAASC,EAAIb,GAIzB,IAHDc,EAAOd,EAAPc,QACAC,EAASf,EAATe,UACAC,EAAQhB,EAARgB,SAGA,OAEEvB,EAAAC,EAAAC,cAAA,OAAKC,UAAU,QACbH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,YACbS,IAAI,UACJC,IAAKQ,IAGPrB,EAAAC,EAAAC,cAAA,OAAKC,UAAU,iBACbH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,eACXmB,GAEHtB,EAAAC,EAAAC,cAAA,KAAGC,UAAU,cACVoB,KCnBI,SAASC,EAAMjB,GAE3B,IADDY,EAAQZ,EAARY,SAGA,OACEnB,EAAAC,EAAAC,cAAA,WAASC,UAAU,gBACjBH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,8BAA4B,UAG1CH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,sCACZgB,ICTM,SAASM,EAAMlB,GAI3B,IAHDmB,EAAInB,EAAJmB,KACAC,EAAKpB,EAALoB,MACAC,EAAGrB,EAAHqB,IAGA,OACE5B,EAAAC,EAAAC,cAAA,OAAKC,UAAU,UACbH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,cACbS,IAAI,SACJC,IAAKe,IAEP5B,EAAAC,EAAAC,cAAA,OAAKC,UAAU,mBACbH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,iBACXuB,GAEH1B,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACVwB,KCnBI,SAASE,IACtB,OACE7B,EAAAC,EAAAC,cAAA,UAAQC,UAAU,UAChBH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,qBAAmB,qECFvB,SAAS2B,EAAIvB,GAAsB,IAAnBY,EAAQZ,EAARY,SAAUY,EAAKxB,EAALwB,MACvC,OACE/B,EAAAC,EAAAC,cAAAF,EAAAC,EAAA+B,SAAA,KACEhC,EAAAC,EAAAC,cAAA,MAAIC,UAAU,uDACX4B,GAEH/B,EAAAC,EAAAC,cAAA,WAASC,UAAU,kBACjBH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,wCACZgB,KCTI,SAASc,EAAS1B,GAAe,IAAZY,EAAQZ,EAARY,SAElC,OACEnB,EAAAC,EAAAC,cAAAF,EAAAC,EAAA+B,SAAA,KACEhC,EAAAC,EAAAC,cAAA,MAAIC,UAAU,sBAAoB,kBAGlCH,EAAAC,EAAAC,cAAA,WAASC,UAAU,aAEjBH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,wBACZgB,KCTI,SAASe,EAAO3B,GAG5B,IAFD4B,EAAO5B,EAAP4B,QACAC,EAAM7B,EAAN6B,OAGA,OAEEpC,EAAAC,EAAAC,cAACE,EAAA,EAAI,CAACC,GAAI+B,GACRpC,EAAAC,EAAAC,cAAA,UAAQa,KAAK,SAASZ,UAAU,mBAC7BgC,ICTM,SAASE,EAAS9B,GAAwB,IAArBY,EAAQZ,EAARY,SAAUgB,EAAO5B,EAAP4B,QAE5C,OACEnC,EAAAC,EAAAC,cAAAF,EAAAC,EAAA+B,SAAA,KACEhC,EAAAC,EAAAC,cAAA,MAAIC,UAAU,yCAAuC,gBACrCgC,GAEhBnC,EAAAC,EAAAC,cAAA,WAASC,UAAU,aACjBH,EAAAC,EAAAC,cAAA,OAAKC,UAAU,wBACZgB,2BCPI,SAASmB,EAAS/B,GAAiB,IAAdgC,EAAIhC,EAAJgC,KAAMC,EAAIjC,EAAJiC,KAExC,OAEExC,EAAAC,EAAAC,cAACuC,EAAA,EAAiB,CAACC,SAAUF,EAAMG,MAAOC,IAAuBC,gBAAgB,QAC9EN,yCCJQ,SAASO,IAEtB,IAAMC,EAAQ,wtIAsId,OAEE/C,EAAAC,EAAAC,cAAA,WAASC,UAAU,QAEjBH,EAAAC,EAAAC,cAAA,WAASC,UAAU,iBACjBH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,iBAAe,mEAC7BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,yUAQ9BH,EAAAC,EAAAC,cAAA,OAAKW,IAAKmC,IACRpC,IAAI,aACJT,UAAU,cAGZH,EAAAC,EAAAC,cAAA,WAASC,UAAU,iBACjBH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,iBAAe,kBAG7BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,eACZH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,gCAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,kBAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,sDAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,uDAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,2DAI9BH,EAAAC,EAAAC,cAAA,WAASC,UAAU,iBACjBH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,itBAK9BH,EAAAC,EAAAC,cAAA,WAASC,UAAU,iBACjBH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,iBAAe,gBAC7BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,qPAG5BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,0BAG5BH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAI,2DACJC,KAAK,UAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,oEAG5BH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAI,mCACJC,KAAK,WAITxC,EAAAC,EAAAC,cAAA,WAASC,UAAU,iBACjBH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,iBAAe,iCAC7BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,6CAC1BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,gZACmXH,EAAAC,EAAAC,cAAA,WAC7YF,EAAAC,EAAAC,cAAA,KAAG+C,KAAK,qDACN9C,UAAU,cAAY,kCAK1BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,iCACIH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,YAAgB,0CAC1EH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,wBAA4B,kCAG1DH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAMQ,EAAM,GACZP,KAAK,WAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,mVAI5BH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAMQ,EAAM,GACZP,KAAK,WAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,+FAG5BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,eACZH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,WAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,mBAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,iBAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,gBAE5BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,0DAG5BH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAI,+BACJC,KAAK,UAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,qFAC8CH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,gBAAoB,yKAE1HH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAMQ,EAAM,GACZP,KAAK,WAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,mTAG5BH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAI,yBACJC,KAAK,WAITxC,EAAAC,EAAAC,cAAA,WAASC,UAAU,iBACjBH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,iBAAe,sBAG7BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,mPAG5BH,EAAAC,EAAAC,cAAA,OACEW,IAAKqC,IACL/C,UAAU,oBAEZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,mDACsBH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,YAAgB,QAAKH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,QAAY,OAAIH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,iBAE3KH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAMQ,EAAM,GACZP,KAAK,WAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,YACjBH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,iBAEvCH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAI,yBACJC,KAAK,UAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,uCACUH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,uBAA2B,gCAG7FH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,eACZH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,oBAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,4BAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,4BAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,2BAC1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,0BAE5BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,+BACEH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,uBAA2B,UAAOH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,yBAA6B,0BACxIH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,qBAEzCH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAI,6BACJC,KAAK,UAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,+DACkCH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,+BAAmC,uEAI/HH,EAAAC,EAAAC,cAAA,WAASC,UAAU,iBACjBH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,iBAAe,yBAG7BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,uHAE1BH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,+BAE9BH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAI,uCACJC,KAAK,UAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,0BAI5BH,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,kDAG1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,kDAG1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,iDAG1BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cAAY,gDAK5BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,kDACqBH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,qBAAyB,OAAIH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,gBAAoB,qCAE1JH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAMQ,EAAM,GACZP,KAAK,WAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,yEAG5BH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAI,oCACJC,KAAK,UAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,4FAI5BH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,eACZH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cACZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,eAAmB,gCAKnDH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cACZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,SAAa,sBAK7CH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cACZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,UAAc,eAM9CH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cACZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,cAAkB,qBAKlDH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cACZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,aAAiB,6BAKjDH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cACZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,aAAiB,6BAKjDH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cACZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,YAAgB,4BAKhDH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cACZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,UAAc,0BAK9CH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cACZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,eAAmB,qBAKnDH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,cACZH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBACXH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,YAAgB,gFAMpDH,EAAAC,EAAAC,cAAA,WAASC,UAAU,iBACjBH,EAAAC,EAAAC,cAAA,MAAIC,UAAU,iBAAe,gBAG7BH,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,mDACsBH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,gBAAoB,8GAA2GH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,gBAAoB,UAAOH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,qBAAyB,sKAEzTH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAMQ,EAAM,GACZP,KAAK,WAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,wIAG5BH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAI,8BACJC,KAAK,UAEPxC,EAAAC,EAAAC,cAAA,KAAGC,UAAU,iBAAe,6BACAH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,qBAAyB,yEAAsEH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,uBAA2B,0DAAuDH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,qBAAyB,oBAAiBH,EAAAC,EAAAC,cAAA,QAAMC,UAAU,cAAY,uBAA2B,mIAElYH,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAMQ,EAAM,GACZP,KAAK,WAGPxC,EAAAC,EAAAC,cAAA,KAAGiD,SAAS,iBAAe,oFAG3BnD,EAAAC,EAAAC,cAACoC,EAAS,CACRC,KAAI,8BACJC,KAAK,uICxbA,SAASY,IAEtB,IAAMC,EAAW,CACf,CACEtB,MAAO,SACPH,IAAK0B,IACLC,KAAM,0IAER,CACExB,MAAO,WACPH,IAAK4B,IACLD,KAAM,qFAER,CACExB,MAAO,QACPH,IAAK6B,IACLF,KAAM,0IAER,CACExB,MAAO,QACPH,IAAK8B,IACLH,KAAM,6JAKJI,EAAa,CACjB,CACEjC,KAAM,oBACNE,IAAKgC,IACLjC,MAAO,wBAET,CACED,KAAM,oBACNE,IAAKiC,IACLlC,MAAO,0CAET,CACED,KAAM,kBACNE,IAAKkC,IACLnC,MAAO,2CAIX,OACE3B,EAAAC,EAAAC,cAAC6D,EAAA,EAAM,KACL/D,EAAAC,EAAAC,cAAC6D,EAAA,EAAK,CAACC,OAAK,EAACC,KAAK,KAChBjE,EAAAC,EAAAC,cAACI,EAAM,CAACE,QAAQ,kBAChBR,EAAAC,EAAAC,cAACO,EAAK,MACNT,EAAAC,EAAAC,cAACgB,EAAK,KACHmC,EAASa,IAAI,SAACC,GACb,OACEnE,EAAAC,EAAAC,cAACkB,EAAI,CACHC,QAAS8C,EAAKvC,IACdN,UAAW6C,EAAKpC,MAChBR,SAAU4C,EAAKZ,UAKvBvD,EAAAC,EAAAC,cAACsB,EAAM,KACJmC,EAAWO,IAAI,SAACC,GACf,OACEnE,EAAAC,EAAAC,cAACuB,EAAM,CACLC,KAAMyC,EAAKzC,KACXE,IAAKuC,EAAKvC,IACVD,MAAOwC,EAAKxC,WAKpB3B,EAAAC,EAAAC,cAAC2B,EAAM,OAGT7B,EAAAC,EAAAC,cAAC6D,EAAA,EAAK,CAACE,KAAK,SACVjE,EAAAC,EAAAC,cAACI,EAAM,CAACE,QAAQ,kBAChBR,EAAAC,EAAAC,cAAC4B,EAAI,KACH9B,EAAAC,EAAAC,cAAC4C,EAAa,OAEhB9C,EAAAC,EAAAC,cAAC2B,EAAM,OAGT7B,EAAAC,EAAAC,cAAC6D,EAAA,EAAK,CAACE,KAAK,cACVjE,EAAAC,EAAAC,cAACI,EAAM,CAACE,QAAQ,cAChBR,EAAAC,EAAAC,cAAC+B,EAAS,KACRjC,EAAAC,EAAAC,cAACgC,EAAO,CACNC,QAAQ,iBACRC,OAAO,eAETpC,EAAAC,EAAAC,cAACgC,EAAO,CACNC,QAAQ,MACRC,OAAO,mBAETpC,EAAAC,EAAAC,cAACgC,EAAO,CACNC,QAAQ,QACRC,OAAO,sBAIXpC,EAAAC,EAAAC,cAAC6D,EAAA,EAAK,CAACC,OAAK,EAACC,KAAK,cAChBjE,EAAAC,EAAAC,cAACmC,EAAS,CAACF,QAAQ,kBACjBnC,EAAAC,EAAAC,cAAA,UAAQ6B,MAAM,eAAeqC,MAAM,OAAOC,OAAO,OAAOxD,IAAI,4IAA4IyD,YAAY,IAAIC,gBAAgB,UAE1OvE,EAAAC,EAAAC,cAAC4B,EAAI,KACH9B,EAAAC,EAAAC,cAAA,KAAGC,UAAU,UAAQ,m/BAMzBH,EAAAC,EAAAC,cAAC6D,EAAA,EAAK,CAACE,KAAK,kBACVjE,EAAAC,EAAAC,cAACmC,EAAS,CAACF,QAAQ,2BACjBnC,EAAAC,EAAAC,cAAA,UAAQ6B,MAAM,gBAAgBqC,MAAM,OAAOC,OAAO,OAAOxD,IAAI,4IAA4IyD,YAAY,IAAIC,gBAAgB,UAE3OvE,EAAAC,EAAAC,cAAC4B,EAAI,KACH9B,EAAAC,EAAAC,cAAA,KAAGC,UAAU,UAAQ,46BAMzBH,EAAAC,EAAAC,cAAC6D,EAAA,EAAK,CAACE,KAAK,oBACVjE,EAAAC,EAAAC,cAACmC,EAAS,CAACF,QAAQ,SACjBnC,EAAAC,EAAAC,cAAA,UAAQ6B,MAAM,kBAAkBqC,MAAM,OAAOC,OAAO,OAAOxD,IAAI,4IAA4IyD,YAAY,IAAIC,gBAAgB,UAE7OvE,EAAAC,EAAAC,cAAC4B,EAAI,KACH9B,EAAAC,EAAAC,cAAA,KAAGC,UAAU,UAAQ,oqBAKzBH,EAAAC,EAAAC,cAAC2B,EAAM,QCzJf,IAYe2C,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC7E,EAAA8E,EAAA,GAAAC,KAAA/E,EAAAgF,KAAA,WAAqBD,KAAK,SAAArE,GAAiD,IAA9CuE,EAAMvE,EAANuE,OAAQC,EAAMxE,EAANwE,OAAQC,EAAMzE,EAANyE,OAAQC,EAAM1E,EAAN0E,OAAQC,EAAO3E,EAAP2E,QAC3DJ,EAAOL,GACPM,EAAON,GACPO,EAAOP,GACPQ,EAAOR,GACPS,EAAQT,MCADU,IAASC,WAAWC,SAASC,eAAe,SACpDC,OACHvF,EAAAC,EAAAC,cAACF,EAAAC,EAAMuF,WAAU,KACfxF,EAAAC,EAAAC,cAACE,EAAA,EAAa,CAACqF,SAAUC,mBACvB1F,EAAAC,EAAAC,cAACkD,EAAG,SAQVoB,wBCnBA7E,EAAAC,QAAiBC,EAAAC,EAAuB,uDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,mDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB","file":"static/js/main.6cdfa51d.chunk.js","sourcesContent":["module.exports = __webpack_public_path__ + \"static/media/about.9fcd71da.png\";","module.exports = __webpack_public_path__ + \"static/media/flowchart_datapipeline.90b16afa.png\";","module.exports = __webpack_public_path__ + \"static/media/azure_key.ffc32eee.png\";","module.exports = __webpack_public_path__ + \"static/media/bi.def290cc.svg\";","module.exports = __webpack_public_path__ + \"static/media/python.6fc4498c.png\";","module.exports = __webpack_public_path__ + \"static/media/azure.5e7b90aa.png\";","module.exports = __webpack_public_path__ + \"static/media/react.70f87bba.png\";","import { Link } from 'react-router-dom';\nimport React from 'react';\n\nexport default function Nav() {\n\n  return (\n    <nav className=\"nav\">\n      <ul className=\"nav__menu\">\n        <li className=\"nav__list\">\n          <Link className=\"nav__link\" to=\"/\">\n            About project\n          </Link>\n        </li>\n        <li className=\"nav__list\">\n          <Link className=\"nav__link\" to=\"/dashboard\">\n            Dashboard\n          </Link>\n        </li>\n        <li className=\"nav__list\">\n          <Link className=\"nav__link\" to=\"/docs\">\n            Documentation\n          </Link>\n        </li>\n      </ul>\n    </nav>\n  )\n}\n","import Nav from '../Nav/Nav';\nimport React from 'react';\n\nexport default function Header({\n  heading\n}) {\n\n  return (\n\n    <header className=\"header\">\n      <h1 className=\"header__heading\">\n        {heading}\n      </h1>\n      <Nav />\n    </header>\n  )\n}\n","import aboutImg from '../../images/about.png';\nimport { useHistory } from 'react-router-dom';\nimport React from 'react';\n\nexport default function About() {\n\n  const history = useHistory();\n\n  const handleButtonClick = () => {\n    history.push('/dashboard')\n  }\n\n  return (\n    <section className=\"about\">\n      <div className='about__container'>\n        <img className='about__img'\n          alt='about'\n          src={aboutImg}\n        />\n        <div className=\"about__content\">\n          <h2 className=\"about__heading\">\n            About this project\n          </h2>\n          <p className=\"about__text\">\n            This project is the part of Advanced practise program of University Of Northumbria.\n            The propose of project is Analysing virtual communication to assess the emotional and behavioural impact of climate change.\n            By Collect Tweet from Twitter and Applying Sentiment Mining techniques to understand and assess users’ opinions and sentiments regarding climate change\n          </p>\n          <p className=\"about__text\">\n            The processing of this project, First we selected top 3 countries that most has flooding disater.\n            Then we using Azure cognative service for Sentiment Analysing and Keyphrases extraction.\n            Finally, We Analyse the result to find user insight anc create the report and dashboard by using Power BI.\n\n          </p>\n          <button className=\"about__button\"\n            type='button'\n            onClick={handleButtonClick}\n          >\n            Go to Dashboard\n          </button>\n        </div>\n      </div>\n    </section >\n  )\n}\n","import React from 'react';\n\nexport default function Tools({\n  children\n}) {\n  return (\n    <section className=\"tools\">\n      <h2 className=\"tools__title\">\n        Technologies\n      </h2>\n      <div className=\"tools__container\">\n        {children}\n      </div>\n    </section>\n  )\n}\n","import React from 'react';\n\nexport default function Card({\n  cardImg,\n  cardTitle,\n  cardDesc\n}) {\n\n  return (\n\n    <div className=\"card\">\n      <img className=\"card__img\"\n        alt=\"cardImg\"\n        src={cardImg}\n      />\n\n      <div className=\"card__content\">\n        <h2 className=\"card__title\">\n          {cardTitle}\n        </h2>\n        <p className=\"card__desc\">\n          {cardDesc}\n        </p>\n      </div>\n\n    </div>\n\n\n  )\n}\n","import React from 'react';\n\nexport default function Member({\n  children\n}) {\n\n  return (\n    <section className=\"tools member\">\n      <h2 className=\"tools__title member__title\">\n        Member\n      </h2>\n      <div className=\"tools__container member__container\">\n        {children}\n      </div>\n    </section>\n  )\n}\n","\nimport React from 'react';\n\nexport default function Avatar({\n  name,\n  major,\n  img\n}) {\n\n  return (\n    <div className=\"avatar\">\n      <img className=\"avatar__img\"\n        alt=\"avater\"\n        src={img}\n      />\n      <div className='avatar__content'>\n        <h2 className=\"avatar__title\">\n          {name}\n        </h2>\n        <p className=\"avatar__major\">\n          {major}\n        </p>\n      </div>\n    </div>\n  )\n}\n","import React from 'react';\nexport default function Footer() {\n  return (\n    <footer className=\"footer\">\n      <p className=\"footer__copyright\">\n        2022 Northumbria Unviersite, Newcastle upon tyne, United Kigndom\n      </p>\n    </footer>\n  )\n}\n","import React from 'react';\n\nexport default function Main({ children, title }) {\n  return (\n    <>\n      <h2 className=\"selection__heading dashboard__heading main__heading\">\n        {title}\n      </h2>\n      <section className=\"dashboard main\">\n        <div className=\"dashboard__container main__container\">\n          {children}\n        </div>\n      </section>\n    </>\n  )\n\n}\n","import React from 'react';\nexport default function Selection({ children }) {\n\n  return (\n    <>\n      <h2 className=\"selection__heading\">\n        Select country\n      </h2>\n      <section className=\"selection\">\n\n        <div className=\"selection__container\">\n          {children}\n        </div>\n      </section>\n    </>\n  )\n}\n","import React from 'react';\nimport { Link } from 'react-router-dom';\nexport default function Country({\n  country,\n  toPath\n}) {\n\n  return (\n\n    <Link to={toPath}>\n      <button type='button' className=\"country__button\">\n        {country}\n      </button >\n    </Link>\n  )\n}\n","import React from 'react';\n\nexport default function Dashboard({ children, country }) {\n\n  return (\n    <>\n      <h2 className=\"selection__heading dashboard__heading\">\n        Dashboard of {country}\n      </h2>\n      <section className=\"dashboard\">\n        <div className=\"dashboard__container\">\n          {children}\n        </div>\n      </section>\n    </>\n  )\n}\n","import SyntaxHighlighter from 'react-syntax-highlighter';\nimport { tomorrowNightEighties } from 'react-syntax-highlighter/dist/esm/styles/hljs';\nimport React from 'react';\n\nexport default function CodeBlock({ code, lang }) {\n\n  return (\n\n    <SyntaxHighlighter language={lang} style={tomorrowNightEighties} showLineNumbers=\"true\">\n      {code}\n    </SyntaxHighlighter>\n  )\n}\n","import React from 'react';\nimport CodeBlock from '../CodeBlock/CodeBlock';\nimport flowChart from '../../images/flowchart_datapipeline.png';\nimport azure from '../../images/azure_key.png';\n\nexport default function Documentation() {\n\n  const codes = [`def scrape_by_geo(keywords, geocode,since,until, outfile):\n    c = twint.Config()\n    c.Search = keywords #search keyword c.Since = since\n    c.Until = until\n    c.Limit = 50000\n    c.Geo = geocode\n    c.Store_json = True\n    c.Output = \"output/\" + outfile\n    c.Hide_output = True\n    c.Count = True\n    c.Stats = True\n    c.Lang = 'en'\n    twint.run.Search(c)`\n    ,\n    `def twint_loop(searchterm, geocode, since, until):\n    dirname = clean_name(\"spain\") # create folder in output folder\n    try:\n    # Create target Directory\n        chdir('output') \n        mkdir(dirname)\n        print(\"Directory\" , dirname ,  \"Created \")\n    except FileExistsError:\n        print(\"Directory\" , dirname ,  \"already exists\")\n\n    daterange = pd.date_range(since, until)\n\n    for start_date in daterange:\n\n        since= start_date.strftime(\"%Y-%m-%d\")\n        until = (start_date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n\n        json_name = '%s.json' % since\n        json_name = path.join(dirname, json_name)\n\n        print('Getting %s ' % since )\n        scrape_by_geo(searchterm,geocode, since, until, json_name)\n`,\n    `import pandas as pd\nimport numpy as np\nimport re\nimport emoji\nimport nltk\nfrom glob import glob\nfrom os import mkdir, path\n\nnltk.download('words')\nwords = set(nltk.corpus.words.words())\n\nfile_names = glob(path.join('spain','*.json')) # edit folder name to folder that you want to covert all JSON file to CSV\ndfs = [pd.read_json(fn, lines = True) for fn in file_names]\ntweet_df = pd.concat(dfs)\n\n\n# cleaner function for remove @ sign, http linke, Emoji and # sign\ndef cleaner(tweet):\n    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n    tweet = \" \".join(tweet.split())\n    tweet = ''.join(c for c in tweet if c not in emoji.distinct_emoji_list(c)) #Remove Emojis\n    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n    tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) \n         if w.lower() in words or not w.isalpha())\n    return tweet\n\n\n# df = pd.read_csv('output/tweets_eu&uk_dataset.csv') #read file \n\ntweet_df = tweet_df[['user_id','username', 'date', 'tweet']]\n\ntweet_df['tweet'] =tweet_df['tweet'].map(lambda x: cleaner(x))\n\ntweet_df['tweet'] = tweet_df['tweet'].astype(\"string\")\n\ntweet_df.info()\n\nnan = float(\"NaN\")\ntweet_df.replace(\"\",nan, inplace=True)\ntweet_df.dropna(subset=['tweet'],inplace=True)\n\ntweet_df.to_csv('tweets_spain_cleaned_dataset.csv') # edit output filename. Change spain to your country working on.`\n    , `subscription_key = '<YOUR_SUBSCRIPTION_KEY>'\nheaders = {\"Ocp-Apim-Subscription-Key\": subscription_key}\nendpoint = \"<YOUR_ENDPOIN_URL>\"\n\nsentiment_url = endpoint + \"/text/analytics/v3.0/sentiment\"`\n    , `subscription_key = '<YOUR_SUBSCRIPTION_KEY>'\nheaders = {\"Ocp-Apim-Subscription-Key\": subscription_key}\nendpoint = \"<YOUR_ENDPOIN_URL>\"\nkeyphrase_url = endpoint + \"/text/analytics/v3.0/keyphrases\"`\n    , `import pandas as pd\n\ndf = pd.read_csv('tweets_spain_cleaned_dataset.csv') # select file that your want to split\n\nhalf_df =len(df) // 2\n\n\ndef split_dataframe_by_position(df, splits):\n    \"\"\"\n    Takes a dataframe and an integer of the number of splits to create.\n    Returns a list of dataframes.\n    \"\"\"\n    dataframes = []\n    index_to_split = len(df) // splits\n    start = 0\n    end = index_to_split\n    for split in range(splits):\n        temporary_df = df.iloc[start:end, :] dataframes.append(temporary_df)\n        start += index_to_split\n        end += index_to_split\n    return dataframes\n\nsplit_dataframes = split_dataframe_by_position(df, 2)\nprint(split_dataframes[0])\n\nsplit_dataframes[0].to_csv('tweets_spain_cleaned_dataset_1.csv') # first half of data\nsplit_dataframes[1].to_csv('tweets_spain_cleaned_dataset_2.csv') # second half of data`\n    , `import pandas as pd\n\n\n# read first half CSV file\ndf1 = pd.read_csv('spain_sentimentCombinedResult_1.csv')\n\n# read second half CSV file\ndf2 = pd.read_csv('spain_sentimentCombinedResult_2.csv')\n\nframes= [df1,df2]\n\nresult = pd.concat(frames)\n\n# export into one CSV file\nresult.to_csv('spain_sentimentCombinedResult.csv')`\n  ]\n\n\n  return (\n\n    <article className=\"docs\">\n\n      <section className=\"docs__section\">\n        <h2 className=\"docs__heading\">Twitter Sentiment and Text Analsis with Azure Cognitive Service</h2>\n        <p className=\"docs__content\">\n          This project aims to determine the impact of climate change on human well-being\n          with a focus on flooding. During the course of this project, sentiment and text\n          analysis are being undertaken using Phyton and Microsoft Azure cognitive services\n          to find out people’s emotions, feelings and opinions caused by this disaster.\n        </p>\n      </section>\n\n      <img src={flowChart}\n        alt=\"flow-chart\"\n        className='docs__img'\n      />\n\n      <section className=\"docs__section\">\n        <h2 className=\"docs__heading\">\n          Data Pipeline:\n        </h2>\n        <ul className=\"docs__lists\">\n          <li className=\"docs__list\">Data collection from Twitter</li>\n          <li className=\"docs__list\">Data cleansing</li>\n          <li className=\"docs__list\">Sentiment analysis (using Azure Cognitive Service)</li>\n          <li className=\"docs__list\">Result Categorisation (Positive, Negative, Neutral)</li>\n          <li className=\"docs__list\">Keyphrases extraction (using Azure Cognative Service)</li>\n        </ul>\n      </section>\n\n      <section className=\"docs__section\">\n        <p className=\"docs__content\">\n          The first step is to query and cleanse the tweets on Twitter, such as deleting the duplicate data, the link, @ sign and hashtag sign. This is because all those elements will make the result of sentiment analysis inaccurate. Following this step, using Azure Cognitive service API to analyse all the tweets. After obtaining the result, all the information must be analysed before moving to the keyphrases extraction stage. The next step is to separate sentiment results into four groups that are 1. Positive 2. Negative 3. Neutral 4. Mixed. After that, using keyphrases extraction from Azure cognitive service API to extract the keyword of each group. Finally, analyse the results and create the dashboard on Power BI.\n        </p>\n      </section>\n\n      <section className=\"docs__section\">\n        <h2 className=\"docs__heading\">Prerequisite</h2>\n        <p className=\"docs__content\">\n          To start the implementation, the Azure account is required. After that, create the resource group. Then generate Language service as part of the Cognitive service since it is crucial in the Sentiment Analysis and Keyphrases Extraction stage.\n        </p>\n        <p className='docs__content'>\n          1. Clone this reposity\n        </p>\n        <CodeBlock\n          code={`$ git clone https://github.com/four88/Vcomms_twitter.git`}\n          lang='shell'\n        />\n        <p className='docs__content'>\n          2. Move to the directory of this folder and rn the below command\n        </p>\n        <CodeBlock\n          code={`$ pip install -r requirement.txt`}\n          lang='shell'\n        />\n      </section>\n\n      <section className=\"docs__section\">\n        <h2 className=\"docs__heading\">Data Collection and Cleansing</h2>\n        <h3 className=\"docs__bold\">With out a Twitter API token(Using Twint)</h3>\n        <p className=\"docs__content\">\n          If the Twitter API token is not accessible, the Twint library can be used instead. However, there is some negative point using Twint, and this is because Twint does not allow querying the tweets on the specific location name. But using the geographic code (Longitude, Latitude) can also be utilised to search for the location. The recommendation to use this website to look for the geographic code<br />\n          <a href=\"https://www.mapdevelopers.com/draw-circle-tool.php\"\n            className='docs__link'\n          >\n            Link to geographic code tools\n          </a>\n        </p>\n        <p className=\"docs__content\">\n          To set the required number on <code className='docs__code'>c. Limit</code>, the number can be found on line 12 of\n          <code className='docs__code'>queryTweetsTwint.py </code>\n          or line 5 on below code block.\n        </p>\n        <CodeBlock\n          code={codes[0]}\n          lang='python'\n        />\n        <p className=\"docs__content\">\n          The next step is to edit the name of the directory folder. All the tweets that query from Twitter will be located in this folder.\n          It is important to note that the folder’s name must relate to the country that will query the tweet. For instance, if the country that will be queried is Spain, the folder’s name should be Spain.\n        </p>\n        <CodeBlock\n          code={codes[1]}\n          lang='python'\n        />\n        <p className=\"docs__content\">\n          After setting the target dictionary folder, the tweet query function needs four parameters.\n        </p>\n        <ul className=\"docs__lists\">\n          <li className=\"docs__list\">Keyword</li>\n          <li className=\"docs__list\">Geographic code</li>\n          <li className=\"docs__list\">Starting date</li>\n          <li className=\"docs__list\">Ending date</li>\n        </ul>\n        <p className=\"docs__content\">\n          Finally, run the below command to run the script file.\n        </p>\n        <CodeBlock\n          code={`$ python3 queryTweetNoApi.py`}\n          lang='shell'\n        />\n        <p className='docs__content'>\n          The JSON file will appear in “output/YOUR-FOLDER_NAME”. Next is to open <code className=\"docs__code\">cleanData.py</code> in the output folder with the text editor. Then edit line 12 of the folder name collected from the JSON files and the output filename on the final line of the code.\n        </p>\n        <CodeBlock\n          code={codes[2]}\n          lang='python'\n        />\n        <p className='docs__content'>\n          After that, run this below command on your terminal to run this file This file will convert all the JSON files to one CSV file. And clean all the data remove @ sign, http linke, Emoji and # sign also delete deplication data. Now you will get CSV file that ready for next step there is Sentiment Analysis\n        </p>\n        <CodeBlock\n          code={`$ python3 cleanData.py`}\n          lang='shell'\n        />\n      </section>\n\n      <section className=\"docs__section\">\n        <h2 className=\"docs__heading\">\n          Sentiment Analysis\n        </h2>\n        <p className=\"docs__content\">\n          This step is to use Azure cognitive service for Sentiment analysis. There is a need to create an Azure account to create a cognitive service (Language Service) on Azure. Then the endpoint URL and keys will appear to connect to the service.\n        </p>\n        <img\n          src={azure}\n          className=\"docs__img_azure\"\n        />\n        <p className='docs__content'>\n          To connect to the Sentiment analysis Azure, add <code className='docs__code'>ENDPOINT</code> and <code className='docs__code'>KEYS</code> in <code className='docs__code'>sentiment.py</code>\n        </p>\n        <CodeBlock\n          code={codes[3]}\n          lang='python'\n        />\n        <p className='docs__content'>\n          Then run <code className='docs__code'>sentiment.py</code>\n        </p>\n        <CodeBlock\n          code={`$ python3 sentiment.py`}\n          lang=\"shell\"\n        />\n        <p className='docs__content'>\n          After run this script, You will get <code className='docs__code'>sentimentResult.csv</code>\n          This table contain 4 columms\n        </p>\n        <ul className='docs__lists'>\n          <li className='docs__list'>Sentiment result</li>\n          <li className='docs__list'>Number of positive words</li>\n          <li className='docs__list'>Number of negative words</li>\n          <li className='docs__list'>Number of neutral words</li>\n          <li className='docs__list'>Number of mixed words</li>\n        </ul>\n        <p className='docs__content'>\n          The next step is to combine <code className='docs__code'>sentimentResult.csv</code>  with <code className='docs__code'>tweetCleanedNoApi.csv</code> by running\n          By running <code className='docs__code'>combineResult.py</code>\n        </p>\n        <CodeBlock\n          code={`$ python3 combineResult.py`}\n          lang=\"shell\"\n        />\n        <p className='docs__content'>\n          Before moving to the next step, the result should appear as <code className='docs__code'>sentimentResultCombined.csv</code>. The file is going to be used to analyse the result of sentiment.\n        </p>\n      </section>\n\n      <section className=\"docs__section\">\n        <h2 className=\"docs__heading\">\n          Keyphrases Extraction\n        </h2>\n        <p className=\"docs__content\">\n          After obtaining the sentiment result, separate the result into three groups (positive, negative, neutral) by running\n          <code className=\"docs__code\">separateSentimentResult.py</code>\n        </p>\n        <CodeBlock\n          code={`$ python3 seperateSentimentresult.py`}\n          lang=\"shell\"\n        />\n        <p className='docs__content'>\n          The output has 4 files\n        </p>\n\n        <ul>\n          <li className='docs__list'>\n            YOUR_COUNTRY_NAME_sentimentResult_Negative.csv\n          </li>\n          <li className='docs__list'>\n            YOUR_COUNTRY_NAME_sentimentResult_Positive.csv\n          </li>\n          <li className='docs__list'>\n            YOUR_COUNTRY_NAME_sentimentResult_Neutral.csv\n          </li>\n          <li className='docs__list'>\n            YOUR_COUNTRY_NAME_sentimentResult_Mixed.csv\n          </li>\n        </ul>\n\n        <p className='docs__content'>\n          Now the Keyphrases are ready. Then, set up the <code className='docs__code'>subscription_key </code>and <code className='docs__code'>ENDPONIT_URL</code>, the same as sentiment analysis.\n        </p>\n        <CodeBlock\n          code={codes[4]}\n          lang='python'\n        />\n        <p className='docs__content'>\n          Run the script to get the result of Keyphrases extraction from Azure.\n        </p>\n        <CodeBlock\n          code={`$ python3 keyphrasesExtraction.py`}\n          lang='shell'\n        />\n        <p className='docs__content'>\n          Then the CSV file will appear and is ready for analytics\n          The table contains nine columns\n        </p>\n        <ul className='docs__lists'>\n          <li className='docs__list'>\n            <p className='docs__content'>\n              <code className='docs__code'>created_at </code>\n              Time that tweet was created\n            </p>\n          </li>\n\n          <li className='docs__list'>\n            <p className='docs__content'>\n              <code className='docs__code'>user </code>\n              Username of tweet\n            </p>\n          </li>\n\n          <li className='docs__list'>\n            <p className='docs__content'>\n              <code className='docs__code'>tweet </code>\n              tweet text\n            </p>\n          </li>\n\n\n          <li className='docs__list'>\n            <p className='docs__content'>\n              <code className='docs__code'>sentiment </code>\n              sentiment result\n            </p>\n          </li>\n\n          <li className='docs__list'>\n            <p className='docs__content'>\n              <code className='docs__code'>positive </code>\n              Number of positive words\n            </p>\n          </li>\n\n          <li className='docs__list'>\n            <p className='docs__content'>\n              <code className='docs__code'>negative </code>\n              Number of negative words\n            </p>\n          </li>\n\n          <li className='docs__list'>\n            <p className='docs__content'>\n              <code className='docs__code'>neutral </code>\n              Number of neutral words\n            </p>\n          </li>\n\n          <li className='docs__list'>\n            <p className='docs__content'>\n              <code className='docs__code'>mixed </code>\n              Number of mixed words\n            </p>\n          </li>\n\n          <li className='docs__list'>\n            <p className='docs__content'>\n              <code className='docs__code'>keyPhrases </code>\n              list of keywords\n            </p>\n          </li>\n\n          <li className='docs__list'>\n            <p className='docs__content'>\n              <code className='docs__code'>keyword </code>\n              explode keyPhrases into one word That maens one tweet can have many rows.\n            </p>\n          </li>\n        </ul>\n      </section>\n      <section className=\"docs__section\">\n        <h2 className='docs__heading'>\n          Handle error\n        </h2>\n        <p className='docs__content'>\n          In the case of an error occurring while running <code className=\"docs__code\">sentiment.py</code>. The recommendation is to separate the CSV file used for sentiment analysis into two parts before running <code className='docs__code'>sentiment.py</code> using <code className=\"docs__code\">splitDataframe.py</code>. To do so, open the file with the text editor and edit line 3 to the data that wants to be split. Next, move to the last 2 lines of code to edit the output name.\n        </p>\n        <CodeBlock\n          code={codes[5]}\n          lang='python'\n        />\n        <p className='docs__content'>\n          Finally, run the below command on the terminal to run the file. The CSV file will appear and is ready to run the Sentiment analysis.\n        </p>\n        <CodeBlock\n          code={`$ python3 splitDataframe.py`}\n          lang='shell'\n        />\n        <p className='docs__content'>\n          Notedly, if the user uses <code className='docs__code'>splitDataframe.py</code> before running the Sentiment analysis, then there is the need to use <code className='docs__code'>combineDataframe.py</code> to combine 2 CSV files into 1 CSV file before running <code className='docs__code'>separateResult.py</code>. To do so, open <code className='docs__code'>combineDataframe.py</code> with the text editor, edit line 5 and line 8 of the file that wants to be combined. Then edit the final line of the file name.\n        </p>\n        <CodeBlock\n          code={codes[6]}\n          lang='python'\n        />\n\n        <p clasName='docs__content'>\n          Finally, run the below command on the terminal, and then 1 CSV file will appear.\n        </p>\n        <CodeBlock\n          code={`python3 combineDateframe.py`}\n          lang='shell'\n        />\n      </section>\n\n    </article>\n\n  );\n}\n","import React from 'react';\nimport Header from '../Header/Header';\nimport About from '../About/About';\nimport Tools from '../Tools/Tools';\nimport Card from '../Card/Card';\nimport { Switch, Route } from \"react-router-dom\";\nimport Member from '../Member/Member';\nimport Avatar from '../Avatar/Avatar';\nimport Footer from '../Footer/Footer';\nimport Main from '../Main/Main';\nimport Selection from '../Selection/Selection';\nimport Country from '../Country/Country';\nimport Dashboard from '../Dashboard/Dashboard';\nimport Documentation from '../Documentation/Documentation';\nimport biImg from '../../images/bi.svg';\nimport pyImg from '../../images/python.png';\nimport azureImg from '../../images/azure.png';\nimport reactImg from '../../images/react.png';\nimport gbengaImg from '../../images/gbenga.png';\nimport tjImg from '../../images/tj.png';\nimport fourImg from '../../images/four.png';\n\nexport default function App() {\n\n  const cardInfo = [\n    {\n      title: \"Python\",\n      img: pyImg,\n      desc: \"Python provide many library that very useful scraping data from twitter. On this project we decide to using Twint for data collection.\"\n    },\n    {\n      title: \"Power Bi\",\n      img: biImg,\n      desc: \"Using Power BI for analyze The result, Find user insigh and create Data dashboard\"\n    },\n    {\n      title: \"Azure\",\n      img: azureImg,\n      desc: \"Azure provide Cognative service for text analytic work. Sentiment Analysis and Keyphrases API are the best that we use on this project\"\n    },\n    {\n      title: \"React\",\n      img: reactImg,\n      desc: \"For present the result of this project, We create a website to show the dashboard from power BI.React is the part that we using for create frontend part\"\n    }\n  ]\n\n\n  const avatarInfo = [\n    {\n      name: \"Pharanyu Chuenjit\",\n      img: fourImg,\n      major: \"MSc Computer Science\"\n    },\n    {\n      name: \"Olateju Olayiwola\",\n      img: tjImg,\n      major: \"MSc Infomation Science (Data Analytic)\"\n    },\n    {\n      name: \"Gbenga Adejuwon\",\n      img: gbengaImg,\n      major: \"MSc Infomation Science (Data Analytic)\"\n    },\n  ]\n\n  return (\n    <Switch>\n      <Route exact path='/'>\n        <Header heading=\"About Project\" />\n        <About />\n        <Tools >\n          {cardInfo.map((item) => {\n            return (\n              <Card\n                cardImg={item.img}\n                cardTitle={item.title}\n                cardDesc={item.desc}\n              />\n            )\n          })}\n        </Tools>\n        <Member>\n          {avatarInfo.map((item) => {\n            return (\n              <Avatar\n                name={item.name}\n                img={item.img}\n                major={item.major}\n              />\n            )\n          })}\n        </Member>\n        <Footer />\n      </Route>\n\n      <Route path='/docs'>\n        <Header heading=\"Documentation\" />\n        <Main>\n          <Documentation />\n        </Main>\n        <Footer />\n      </Route>\n\n      <Route path='/dashboard'>\n        <Header heading=\"Dashboard\" />\n        <Selection>\n          <Country\n            country=\"United Kingdom\"\n            toPath=\"/dashboard\"\n          />\n          <Country\n            country=\"USA\"\n            toPath=\"/dashboard/usa\"\n          />\n          <Country\n            country=\"Spain\"\n            toPath=\"/dashboard/spain\"\n          />\n        </Selection>\n\n        <Route exact path='/dashboard'>\n          <Dashboard country=\"United Kingdom\">\n            <iframe title=\"Dashboard UK\" width=\"100%\" height=\"100%\" src=\"https://app.powerbi.com/reportEmbed?reportId=40c6a97b-1356-47e6-b75e-d494fd40eea4&autoAuth=true&ctid=e757cfdd-1f35-4457-af8f-7c9c6b1437e3\" frameborder=\"0\" allowFullScreen=\"true\"></iframe>\n          </Dashboard>\n          <Main >\n            <p className='report'>\n              Top in the list of negative keywords are flood, government, climate change, land, country, storm, and rainfall. Top in the list of positive keywords are flood, river, land, project, flood resilience, drainage and government. Top in the list of mixed keywords are flood, government, safety, country, bad weather, wildlife, solution and disaster. An analysis of the tweets show that the contents of the tweet is in line with the sentiments. Also, government is a common keyword which shows people who tweeted holds the government accountable for the flood which has occured and expect so much from the government. An analysis of the sentiment shows that the negative sentiments have the highest count followed by Neutral and then positive. This is expected as flood is not pleasant and the effect is usually negative. Taking a closer look at the sentiments, the trend has been on the decrease from the begining of the period with lowest being quarter 3 of 2022 for positive, neutral and negative sentiments.\n            </p>\n          </Main>\n        </Route>\n\n        <Route path='/dashboard/usa'>\n          <Dashboard country=\"United State of America\">\n            <iframe title=\"Dashboard USA\" width=\"100%\" height=\"100%\" src=\"https://app.powerbi.com/reportEmbed?reportId=46376b84-294c-4a9c-a680-5785e41275bd&autoAuth=true&ctid=e757cfdd-1f35-4457-af8f-7c9c6b1437e3\" frameborder=\"0\" allowFullScreen=\"true\"></iframe>\n          </Dashboard>\n          <Main >\n            <p className='report'>\n              Top on the list of keywords which is common to positive, negative keywords is the flood. The frequency of the flood word-count is significantly much as compared to other keywords across-board. For negative keyword, top on the list following flood is storm, damage, emergency, rainfall, climate change, government and business. For positive keywords, top on the list of the keywords is media, insurance, air ambulance, project, report, government, debris and drainage. For neutral keywords, top on the list are warning,report, new, update, location, safety and flood alert. An anlysis of the tweets shows that the content is in line with the sentiment. For the sentiment result analysis, the counts for negative, positive and neutral are very close percentage wise with negative having 34%, positive 34% and neutral 31%. For the period under review, the sentiment counts per quarter has not had a regular partern all through the period\n            </p>\n          </Main>\n        </Route>\n\n        <Route path='/dashboard/spain'>\n          <Dashboard country=\"Spain\">\n            <iframe title=\"Dashboard Spain\" width=\"100%\" height=\"100%\" src=\"https://app.powerbi.com/reportEmbed?reportId=8ba28e29-3bdf-46e8-921c-414b167149f6&autoAuth=true&ctid=e757cfdd-1f35-4457-af8f-7c9c6b1437e3\" frameborder=\"0\" allowFullScreen=\"true\"></iframe>\n          </Dashboard>\n          <Main>\n            <p className='report'>\n              For the keywords, flood is top on the list and common to negative, positive and neutral keywords. For negative keywords, top on the list are damage, storm, government, climate change, rainfall and emergency. For positive keywords, top on the list are rain, people, support, media, government and wildfire.  Analysin the tweets as well, it shows that they are in line with the keywords and then sentiment. Analysing the sentiment shows that the negative sentiment is top on the list with 37% followed closely by positive sentiments with 34% and then neutral by 28%. Over the period, an analysis of the sentiments show that 2017 had the highest with the lowest being 2011.\n            </p>\n          </Main>\n        </Route>\n        <Footer />\n\n      </Route>\n\n    </Switch >\n\n  );\n}\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom/client';\nimport './index.css';\nimport App from './components/App/App';\nimport reportWebVitals from './reportWebVitals';\nimport { BrowserRouter } from 'react-router-dom';\n\nconst root = ReactDOM.createRoot(document.getElementById('root'));\nroot.render(\n  <React.StrictMode>\n    <BrowserRouter basename={process.env.PUBLIC_URL}>\n      <App />\n    </BrowserRouter>\n  </React.StrictMode>\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n","module.exports = __webpack_public_path__ + \"static/media/gbenga.8b422a4d.png\";","module.exports = __webpack_public_path__ + \"static/media/tj.abd81c27.png\";","module.exports = __webpack_public_path__ + \"static/media/four.6fa3bb72.png\";"],"sourceRoot":""}