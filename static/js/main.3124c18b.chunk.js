(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{17:function(e,t,a){e.exports=a.p+"static/media/about.9fcd71da.png"},22:function(e,t,a){e.exports=a.p+"static/media/flowchart_datapipeline.90b16afa.png"},23:function(e,t,a){e.exports=a.p+"static/media/azure_key.ffc32eee.png"},24:function(e,t,a){e.exports=a.p+"static/media/bi.def290cc.svg"},25:function(e,t,a){e.exports=a.p+"static/media/python.6fc4498c.png"},26:function(e,t,a){e.exports=a.p+"static/media/azure.5e7b90aa.png"},27:function(e,t,a){e.exports=a.p+"static/media/react.70f87bba.png"},279:function(e,t,a){"use strict";a.r(t);var n=a(0),s=a.n(n),c=a(15),o=a.n(c),r=(a(40),a(6));function l(){return s.a.createElement("nav",{className:"nav"},s.a.createElement("ul",{className:"nav__menu"},s.a.createElement("li",{className:"nav__list"},s.a.createElement(r.b,{className:"nav__link",to:"/"},"About project")),s.a.createElement("li",{className:"nav__list"},s.a.createElement(r.b,{className:"nav__link",to:"/dashboard"},"Dashboard")),s.a.createElement("li",{className:"nav__list"},s.a.createElement(r.b,{className:"nav__link",to:"/docs"},"Documentation"))))}function i(e){var t=e.heading;return s.a.createElement("header",{className:"header"},s.a.createElement("h1",{className:"header__heading"},t),s.a.createElement(l,null))}var m=a(17),d=a.n(m),_=a(1);function p(){var e=Object(_.f)();return s.a.createElement("section",{className:"about"},s.a.createElement("div",{className:"about__container"},s.a.createElement("img",{className:"about__img",alt:"about",src:d.a}),s.a.createElement("div",{className:"about__content"},s.a.createElement("h2",{className:"about__heading"},"About this project"),s.a.createElement("p",{className:"about__text"},"This project is the part of Advanced practise program of University Of Northumbria. The propose of project is Analysing virtual communication to assess the emotional and behavioural impact of climate change. By Collect Tweet from Twitter and Applying Sentiment Mining techniques to understand and assess users\u2019 opinions and sentiments regarding climate change"),s.a.createElement("p",{className:"about__text"},"The processing of this project, First we selected top 3 countries that most has flooding disater. Then we using Azure cognative service for Sentiment Analysing and Keyphrases extraction. Finally, We Analyse the result to find user insight anc create the report and dashboard by using Power BI."),s.a.createElement("button",{className:"about__button",type:"button",onClick:function(){e.push("/dashboard")}},"Go to Dashboard"))))}function u(e){var t=e.children;return s.a.createElement("section",{className:"tools"},s.a.createElement("h2",{className:"tools__title"},"Technologies"),s.a.createElement("div",{className:"tools__container"},t))}function h(e){var t=e.cardImg,a=e.cardTitle,n=e.cardDesc;return s.a.createElement("div",{className:"card"},s.a.createElement("img",{className:"card__img",alt:"cardImg",src:t}),s.a.createElement("div",{className:"card__content"},s.a.createElement("h2",{className:"card__title"},a),s.a.createElement("p",{className:"card__desc"},n)))}function f(e){var t=e.children;return s.a.createElement("section",{className:"tools member"},s.a.createElement("h2",{className:"tools__title member__title"},"Member"),s.a.createElement("div",{className:"tools__container member__container"},t))}function E(e){var t=e.name,a=e.major,n=e.img;return s.a.createElement("div",{className:"avatar"},s.a.createElement("img",{className:"avatar__img",alt:"avater",src:n}),s.a.createElement("div",{className:"avatar__content"},s.a.createElement("h2",{className:"avatar__title"},t),s.a.createElement("p",{className:"avatar__major"},a)))}function g(){return s.a.createElement("footer",{className:"footer"},s.a.createElement("p",{className:"footer__copyright"},"2022 Northumbria Unviersite, Newcastle upon tyne, United Kigndom"))}function N(e){var t=e.children,a=e.title;return s.a.createElement(s.a.Fragment,null,s.a.createElement("h2",{className:"selection__heading dashboard__heading main__heading"},a),s.a.createElement("section",{className:"dashboard main"},s.a.createElement("div",{className:"dashboard__container main__container"},t)))}function w(e){var t=e.children;return s.a.createElement(s.a.Fragment,null,s.a.createElement("h2",{className:"selection__heading"},"Select country"),s.a.createElement("section",{className:"selection"},s.a.createElement("div",{className:"selection__container"},t)))}function b(e){var t=e.country,a=e.toPath;return s.a.createElement(r.b,{to:a},s.a.createElement("button",{type:"button",className:"country__button"},t))}function y(e){var t=e.children,a=e.country;return s.a.createElement(s.a.Fragment,null,s.a.createElement("h2",{className:"selection__heading dashboard__heading"},"Dashboard of ",a),s.a.createElement("section",{className:"dashboard"},s.a.createElement("div",{className:"dashboard__container"},t)))}var v=a(284),T=a(282);function S(e){var t=e.code,a=e.lang;return s.a.createElement(v.a,{language:a,style:T.a,showLineNumbers:"true"},t)}var A=a(22),x=a.n(A),k=a(23),R=a.n(k);function C(){var e=["def scrape_by_geo(keywords, geocode,since,until, outfile):\n    c = twint.Config()\n    c.Search = keywords #search keyword c.Since = since\n    c.Until = until\n    c.Limit = 50000\n    c.Geo = geocode\n    c.Store_json = True\n    c.Output = \"output/\" + outfile\n    c.Hide_output = True\n    c.Count = True\n    c.Stats = True\n    c.Lang = 'en'\n    twint.run.Search(c)",'def twint_loop(searchterm, geocode, since, until):\n    dirname = clean_name("spain") # create folder in output folder\n    try:\n    # Create target Directory\n        chdir(\'output\') \n        mkdir(dirname)\n        print("Directory" , dirname ,  "Created ")\n    except FileExistsError:\n        print("Directory" , dirname ,  "already exists")\n\n    daterange = pd.date_range(since, until)\n\n    for start_date in daterange:\n\n        since= start_date.strftime("%Y-%m-%d")\n        until = (start_date + timedelta(days=1)).strftime("%Y-%m-%d")\n\n        json_name = \'%s.json\' % since\n        json_name = path.join(dirname, json_name)\n\n        print(\'Getting %s \' % since )\n        scrape_by_geo(searchterm,geocode, since, until, json_name)\n',"import pandas as pd\nimport numpy as np\nimport re\nimport emoji\nimport nltk\nfrom glob import glob\nfrom os import mkdir, path\n\nnltk.download('words')\nwords = set(nltk.corpus.words.words())\n\nfile_names = glob(path.join('spain','*.json')) # edit folder name to folder that you want to covert all JSON file to CSV\ndfs = [pd.read_json(fn, lines = True) for fn in file_names]\ntweet_df = pd.concat(dfs)\n\n\n# cleaner function for remove @ sign, http linke, Emoji and # sign\ndef cleaner(tweet):\n    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n    tweet = re.sub(r\"(?:@|http?://|https?://|www)S+\", \"\", tweet) #Remove http links\n    tweet = \" \".join(tweet.split())\n    tweet = ''.join(c for c in tweet if c not in emoji.distinct_emoji_list(c)) #Remove Emojis\n    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n    tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) \n         if w.lower() in words or not w.isalpha())\n    return tweet\n\n\n# df = pd.read_csv('output/tweets_eu&uk_dataset.csv') #read file \n\ntweet_df = tweet_df[['user_id','username', 'date', 'tweet']]\n\ntweet_df['tweet'] =tweet_df['tweet'].map(lambda x: cleaner(x))\n\ntweet_df['tweet'] = tweet_df['tweet'].astype(\"string\")\n\ntweet_df.info()\n\nnan = float(\"NaN\")\ntweet_df.replace(\"\",nan, inplace=True)\ntweet_df.dropna(subset=['tweet'],inplace=True)\n\ntweet_df.to_csv('tweets_spain_cleaned_dataset.csv') # edit output filename. Change spain to your country working on.",'subscription_key = \'<YOUR_SUBSCRIPTION_KEY>\'\nheaders = {"Ocp-Apim-Subscription-Key": subscription_key}\nendpoint = "<YOUR_ENDPOIN_URL>"\n\nsentiment_url = endpoint + "/text/analytics/v3.0/sentiment"','subscription_key = \'<YOUR_SUBSCRIPTION_KEY>\'\nheaders = {"Ocp-Apim-Subscription-Key": subscription_key}\nendpoint = "<YOUR_ENDPOIN_URL>"\nkeyphrase_url = endpoint + "/text/analytics/v3.0/keyphrases"',"import pandas as pd\n\ndf = pd.read_csv('tweets_spain_cleaned_dataset.csv') # select file that your want to split\n\nhalf_df =len(df) // 2\n\n\ndef split_dataframe_by_position(df, splits):\n    \"\"\"\n    Takes a dataframe and an integer of the number of splits to create.\n    Returns a list of dataframes.\n    \"\"\"\n    dataframes = []\n    index_to_split = len(df) // splits\n    start = 0\n    end = index_to_split\n    for split in range(splits):\n        temporary_df = df.iloc[start:end, :] dataframes.append(temporary_df)\n        start += index_to_split\n        end += index_to_split\n    return dataframes\n\nsplit_dataframes = split_dataframe_by_position(df, 2)\nprint(split_dataframes[0])\n\nsplit_dataframes[0].to_csv('tweets_spain_cleaned_dataset_1.csv') # first half of data\nsplit_dataframes[1].to_csv('tweets_spain_cleaned_dataset_2.csv') # second half of data","import pandas as pd\n\n\n# read first half CSV file\ndf1 = pd.read_csv('spain_sentimentCombinedResult_1.csv')\n\n# read second half CSV file\ndf2 = pd.read_csv('spain_sentimentCombinedResult_2.csv')\n\nframes= [df1,df2]\n\nresult = pd.concat(frames)\n\n# export into one CSV file\nresult.to_csv('spain_sentimentCombinedResult.csv')"];return s.a.createElement("article",{className:"docs"},s.a.createElement("section",{className:"docs__section"},s.a.createElement("h2",{className:"docs__heading"},"Twitter Sentiment and Text Analsis with Azure Cognitive Service"),s.a.createElement("p",{className:"docs__content"},"This project aims to determine the impact of climate change on human well-being with a focus on flooding. During the course of this project, sentiment and text analysis are being undertaken using Phyton and Microsoft Azure cognitive services to find out people\u2019s emotions, feelings and opinions caused by this disaster.")),s.a.createElement("img",{src:x.a,alt:"flow-chart",className:"docs__img"}),s.a.createElement("section",{className:"docs__section"},s.a.createElement("h2",{className:"docs__heading"},"Data Pipeline:"),s.a.createElement("ul",{className:"docs__lists"},s.a.createElement("li",{className:"docs__list"},"Data collection from Twitter"),s.a.createElement("li",{className:"docs__list"},"Data cleansing"),s.a.createElement("li",{className:"docs__list"},"Sentiment analysis (using Azure Cognitive Service)"),s.a.createElement("li",{className:"docs__list"},"Result Categorisation (Positive, Negative, Neutral)"))),s.a.createElement("section",{className:"docs__section"},s.a.createElement("p",{className:"docs__content"},"The first step is to query and cleanse the tweets on Twitter, such as deleting the duplicate data, the link, @ sign and hashtag sign. This is because all those elements will make the result of sentiment analysis inaccurate. Following this step, using Azure Cognitive service API to analyse all the tweets. After obtaining the result, all the information must be analysed before moving to the keyphrases extraction stage. The next step is to separate sentiment results into four groups that are 1. Positive 2. Negative 3. Neutral 4. Mixed. After that, using keyphrases extraction from Azure cognitive service API to extract the keyword of each group. Finally, analyse the results and create the dashboard on Power BI.")),s.a.createElement("section",{className:"docs__section"},s.a.createElement("h2",{className:"docs__heading"},"Prerequisite"),s.a.createElement("p",{className:"docs__content"},"To start the implementation, the Azure account is required. After that, create the resource group. Then generate Language service as part of the Cognitive service since it is crucial in the Sentiment Analysis and Keyphrases Extraction stage."),s.a.createElement("p",{className:"docs__content"},"1. Clone this reposity"),s.a.createElement(S,{code:"$ git clone https://github.com/four88/Vcomms_twitter.git",lang:"shell"}),s.a.createElement("p",{className:"docs__content"},"2. Move to the directory of this folder and rn the below command"),s.a.createElement(S,{code:"$ pip install -r requirement.txt",lang:"shell"})),s.a.createElement("section",{className:"docs__section"},s.a.createElement("h2",{className:"docs__heading"},"Data Collection and Cleansing"),s.a.createElement("h3",{className:"docs__bold"},"With out a Twitter API token(Using Twint)"),s.a.createElement("p",{className:"docs__content"},"If the Twitter API token is not accessible, the Twint library can be used instead. However, there is some negative point using Twint, and this is because Twint does not allow querying the tweets on the specific location name. But using the geographic code (Longitude, Latitude) can also be utilised to search for the location. The recommendation to use this website to look for the geographic code",s.a.createElement("br",null),s.a.createElement("a",{href:"https://www.mapdevelopers.com/draw-circle-tool.php",className:"docs__link"},"Link to geographic code tools")),s.a.createElement("p",{className:"docs__content"},"To set the required number on ",s.a.createElement("code",{className:"docs__code"},"c. Limit"),", the number can be found on line 12 of",s.a.createElement("code",{className:"docs__code"},"queryTweetsTwint.py "),"or line 5 on below code block."),s.a.createElement(S,{code:e[0],lang:"python"}),s.a.createElement("p",{className:"docs__content"},"The next step is to edit the name of the directory folder. All the tweets that query from Twitter will be located in this folder. It is important to note that the folder\u2019s name must relate to the country that will query the tweet. For instance, if the country that will be queried is Spain, the folder\u2019s name should be Spain."),s.a.createElement(S,{code:e[1],lang:"python"}),s.a.createElement("p",{className:"docs__content"},"After setting the target dictionary folder, the tweet query function needs four parameters."),s.a.createElement("ul",{className:"docs__lists"},s.a.createElement("li",{className:"docs__list"},"Keyword"),s.a.createElement("li",{className:"docs__list"},"Geographic code"),s.a.createElement("li",{className:"docs__list"},"Starting date"),s.a.createElement("li",{className:"docs__list"},"Ending date")),s.a.createElement("p",{className:"docs__content"},"Finally, run the below command to run the script file."),s.a.createElement(S,{code:"$ python3 queryTweetNoApi.py",lang:"shell"}),s.a.createElement("p",{className:"docs__content"},"The JSON file will appear in \u201coutput/YOUR-FOLDER_NAME\u201d. Next is to open ",s.a.createElement("code",{className:"docs__code"},"cleanData.py")," in the output folder with the text editor. Then edit line 12 of the folder name collected from the JSON files and the output filename on the final line of the code."),s.a.createElement(S,{code:e[2],lang:"python"}),s.a.createElement("p",{className:"docs__content"},"After that, run this below command on your terminal to run this file This file will convert all the JSON files to one CSV file. And clean all the data remove @ sign, http linke, Emoji and # sign also delete deplication data. Now you will get CSV file that ready for next step there is Sentiment Analysis"),s.a.createElement(S,{code:"$ python3 cleanData.py",lang:"shell"})),s.a.createElement("section",{className:"docs__section"},s.a.createElement("h2",{className:"docs__heading"},"Sentiment Analysis"),s.a.createElement("p",{className:"docs__content"},"This step is to use Azure cognitive service for Sentiment analysis. There is a need to create an Azure account to create a cognitive service (Language Service) on Azure. Then the endpoint URL and keys will appear to connect to the service."),s.a.createElement("img",{src:R.a,className:"docs__img_azure"}),s.a.createElement("p",{className:"docs__content"},"To connect to the Sentiment analysis Azure, add ",s.a.createElement("code",{className:"docs__code"},"ENDPOINT")," and ",s.a.createElement("code",{className:"docs__code"},"KEYS")," in ",s.a.createElement("code",{className:"docs__code"},"sentiment.py")),s.a.createElement(S,{code:e[3],lang:"python"}),s.a.createElement("p",{className:"docs__content"},"Then run ",s.a.createElement("code",{className:"docs__code"},"sentiment.py")),s.a.createElement(S,{code:"$ python3 sentiment.py",lang:"shell"}),s.a.createElement("p",{className:"docs__content"},"After run this script, You will get ",s.a.createElement("code",{className:"docs__code"},"sentimentResult.csv"),"This table contain 4 columms"),s.a.createElement("ul",{className:"docs__lists"},s.a.createElement("li",{className:"docs__list"},"Sentiment result"),s.a.createElement("li",{className:"docs__list"},"Number of positive words"),s.a.createElement("li",{className:"docs__list"},"Number of negative words"),s.a.createElement("li",{className:"docs__list"},"Number of neutral words"),s.a.createElement("li",{className:"docs__list"},"Number of mixed words")),s.a.createElement("p",{className:"docs__content"},"The next step is to combine ",s.a.createElement("code",{className:"docs__code"},"sentimentResult.csv"),"  with ",s.a.createElement("code",{className:"docs__code"},"tweetCleanedNoApi.csv")," by running By running ",s.a.createElement("code",{className:"docs__code"},"combineResult.py")),s.a.createElement(S,{code:"$ python3 combineResult.py",lang:"shell"}),s.a.createElement("p",{className:"docs__content"},"Before moving to the next step, the result should appear as ",s.a.createElement("code",{className:"docs__code"},"sentimentResultCombined.csv"),". The file is going to be used to analyse the result of sentiment.")),s.a.createElement("section",{className:"docs__section"},s.a.createElement("h2",{className:"docs__heading"},"Keyphrases Extraction"),s.a.createElement("p",{className:"docs__content"},"After obtaining the sentiment result, separate the result into three groups (positive, negative, neutral) by running",s.a.createElement("code",{className:"docs__code"},"separateSentimentResult.py")),s.a.createElement(S,{code:"$ python3 seperateSentimentresult.py",lang:"shell"}),s.a.createElement("p",{className:"docs__content"},"The output has 4 files"),s.a.createElement("ul",null,s.a.createElement("li",{className:"docs__list"},"YOUR_COUNTRY_NAME_sentimentResult_Negative.csv"),s.a.createElement("li",{className:"docs__list"},"YOUR_COUNTRY_NAME_sentimentResult_Positive.csv"),s.a.createElement("li",{className:"docs__list"},"YOUR_COUNTRY_NAME_sentimentResult_Neutral.csv"),s.a.createElement("li",{className:"docs__list"},"YOUR_COUNTRY_NAME_sentimentResult_Mixed.csv")),s.a.createElement("p",{className:"docs__content"},"Now the Keyphrases are ready. Then, set up the ",s.a.createElement("code",{className:"docs__code"},"subscription_key "),"and ",s.a.createElement("code",{className:"docs__code"},"ENDPONIT_URL"),", the same as sentiment analysis."),s.a.createElement(S,{code:e[4],lang:"python"}),s.a.createElement("p",{className:"docs__content"},"Run the script to get the result of Keyphrases extraction from Azure."),s.a.createElement(S,{code:"$ python3 keyphrasesExtraction.py",lang:"shell"}),s.a.createElement("p",{className:"docs__content"},"Then the CSV file will appear and is ready for analytics The table contains ten columns"),s.a.createElement("ul",{className:"docs__lists"},s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"created_at "),"Time that tweet was created")),s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"user "),"Username of tweet")),s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"tweet "),"tweet text")),s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"location "),"location of tweet")),s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"sentiment "),"sentiment result")),s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"positive "),"Number of positive words")),s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"negative "),"Number of negative words")),s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"neutral "),"Number of neutral words")),s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"mixed "),"Number of mixed words")),s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"keyPhrases "),"list of keywords")),s.a.createElement("li",{className:"docs__list"},s.a.createElement("p",{className:"docs__content"},s.a.createElement("code",{className:"docs__code"},"keyword "),"explode keyPhrases into one word That maens one tweet can have many rows.")))),s.a.createElement("section",{className:"docs__section"},s.a.createElement("h2",{className:"docs__heading"},"Handle error"),s.a.createElement("p",{className:"docs__content"},"In the case of an error occurring while running ",s.a.createElement("code",{className:"docs__code"},"sentiment.py"),". The recommendation is to separate the CSV file used for sentiment analysis into two parts before running ",s.a.createElement("code",{className:"docs__code"},"sentiment.py")," using ",s.a.createElement("code",{className:"docs__code"},"splitDataframe.py"),". To do so, open the file with the text editor and edit line 3 to the data that wants to be split. Next, move to the last 2 lines of code to edit the output name."),s.a.createElement(S,{code:e[4],lang:"python"}),s.a.createElement("p",{className:"docs__content"},"Finally, run the below command on the terminal to run the file. The CSV file will appear and is ready to run the Sentiment analysis."),s.a.createElement(S,{code:"$ python3 splitDataframe.py",lang:"shell"}),s.a.createElement("p",{className:"docs__content"},"Notedly, if the user uses ",s.a.createElement("code",{className:"docs__code"},"splitDataframe.py")," before running the Sentiment analysis, then there is the need to use ",s.a.createElement("code",{className:"docs__code"},"combineDataframe.py")," to combine 2 CSV files into 1 CSV file before running ",s.a.createElement("code",{className:"docs__code"},"separateResult.py"),". To do so, open ",s.a.createElement("code",{className:"docs__code"},"combineDataframe.py")," with the text editor, edit line 5 and line 8 of the file that wants to be combined. Then edit the final line of the file name."),s.a.createElement(S,{code:e[5],lang:"python"}),s.a.createElement("p",{clasName:"docs__content"},"Finally, run the below command on the terminal, and then 1 CSV file will appear."),s.a.createElement(S,{code:"python3 combineDateframe.py",lang:"shell"})))}var j=a(24),D=a.n(j),U=a(25),P=a.n(U),O=a(26),I=a.n(O),F=a(27),z=a.n(F),Y=a(28),K=a.n(Y),L=a(29),M=a.n(L),B=a(30),q=a.n(B);function V(){var e=[{title:"Python",img:P.a,desc:"Python provide many library that very useful scraping data from twitter. On this project we decide to using Twint for data collection."},{title:"Power Bi",img:D.a,desc:"Using Power BI for analyze The result, Find user insigh and create Data dashboard"},{title:"Azure",img:I.a,desc:"Azure provide Cognative service for text analytic work. Sentiment Analysis and Keyphrases API are the best that we use on this project"},{title:"React",img:z.a,desc:"For present the result of this project, We create a website to show the dashboard from power BI.React is the part that we using for create frontend part"}],t=[{name:"Pharanyu Chuenjit",img:q.a,major:"MSc Computer Science"},{name:"Olateju Olayiwola",img:M.a,major:"MSc Infomation Science (Data Analytic)"},{name:"Gbenga Adejuwon",img:K.a,major:"MSc Infomation Science (Data Analytic)"}];return s.a.createElement(_.c,null,s.a.createElement(_.a,{exact:!0,path:"/"},s.a.createElement(i,{heading:"About Project"}),s.a.createElement(p,null),s.a.createElement(u,null,e.map(function(e){return s.a.createElement(h,{cardImg:e.img,cardTitle:e.title,cardDesc:e.desc})})),s.a.createElement(f,null,t.map(function(e){return s.a.createElement(E,{name:e.name,img:e.img,major:e.major})})),s.a.createElement(g,null)),s.a.createElement(_.a,{path:"/docs"},s.a.createElement(i,{heading:"Documentation"}),s.a.createElement(N,null,s.a.createElement(C,null)),s.a.createElement(g,null)),s.a.createElement(_.a,{path:"/dashboard"},s.a.createElement(i,{heading:"Dashboard"}),s.a.createElement(w,null,s.a.createElement(b,{country:"United Kingdom",toPath:"/dashboard"}),s.a.createElement(b,{country:"USA",toPath:"/dashboard/usa"}),s.a.createElement(b,{country:"Spain",toPath:"/dashboard/spain"})),s.a.createElement(_.a,{exact:!0,path:"/dashboard"},s.a.createElement(y,{country:"United Kingdom"},s.a.createElement("iframe",{title:"Dashboard UK",width:"100%",height:"100%",src:"https://app.powerbi.com/reportEmbed?reportId=40c6a97b-1356-47e6-b75e-d494fd40eea4&autoAuth=true&ctid=e757cfdd-1f35-4457-af8f-7c9c6b1437e3",frameborder:"0",allowFullScreen:"true"})),s.a.createElement(N,{title:"Report"},"Based on the results of the sentiment analysis, it can be deduced that Twitter users from the United Kingdom are skewed towards negative tweets with counts taking as much as 59.52%. Top keywords from the analysis also support this deduction")),s.a.createElement(_.a,{path:"/dashboard/usa"},s.a.createElement(y,{country:"United State of America"},s.a.createElement("iframe",{title:"Dashboard USA",width:"100%",height:"100%",src:"https://app.powerbi.com/reportEmbed?reportId=46376b84-294c-4a9c-a680-5785e41275bd&autoAuth=true&ctid=e757cfdd-1f35-4457-af8f-7c9c6b1437e3",frameborder:"0",allowFullScreen:"true"})),s.a.createElement(N,{title:"Report"},"USA's content")),s.a.createElement(_.a,{path:"/dashboard/spain"},s.a.createElement(y,{country:"Spain"},s.a.createElement("iframe",{title:"Dashboard Spain",width:"100%",height:"100%",src:"https://app.powerbi.com/reportEmbed?reportId=8ba28e29-3bdf-46e8-921c-414b167149f6&autoAuth=true&ctid=e757cfdd-1f35-4457-af8f-7c9c6b1437e3",frameborder:"0",allowFullScreen:"true"})),s.a.createElement(N,{title:"Report"},"Spain's content")),s.a.createElement(g,null)))}var $=function(e){e&&e instanceof Function&&a.e(1).then(a.bind(null,283)).then(function(t){var a=t.getCLS,n=t.getFID,s=t.getFCP,c=t.getLCP,o=t.getTTFB;a(e),n(e),s(e),c(e),o(e)})};o.a.createRoot(document.getElementById("root")).render(s.a.createElement(s.a.StrictMode,null,s.a.createElement(r.a,null,s.a.createElement(V,null)))),$()},28:function(e,t,a){e.exports=a.p+"static/media/gbenga.8b422a4d.png"},29:function(e,t,a){e.exports=a.p+"static/media/tj.abd81c27.png"},30:function(e,t,a){e.exports=a.p+"static/media/four.6fa3bb72.png"},31:function(e,t,a){e.exports=a(279)},40:function(e,t,a){}},[[31,3,2]]]);
//# sourceMappingURL=main.3124c18b.chunk.js.map